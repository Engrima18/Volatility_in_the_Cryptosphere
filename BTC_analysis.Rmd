---
title: | 
  | **Bitcoin volatility analysis**
  | 
author: | 
  | _Author_:
  | Enrico Grimaldi, 1884443
  |
  | _Professor_:
  | Luca Tardella
  |
date: | 
  |
  | La Sapienza University of Rome
  | a.y. 2022/2023
  |
  | Final project for the course of
  | _Statistical Methods for Data Science 2_
output: 
  pdf_document: default
  html_document: default
toc: no
number_sections: true
bibliography: bibliography.bib
biblio-style: "apalike"
fontsize: 12pt
header-includes:
  - \usepackage{adjustbox}
  - \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, results = "hide")
```

```{r}
# import libraries 
library(tidyverse)
library(gt)
library(summarytools)
library(gridExtra)
library(ggplot2)
library(corrplot)
library(reshape2)
library(mcmcplots)
library(caret)
library(ggpubr)
library(fitdistrplus)
library(viridis)
library(tseries)
library(nortsTest)
library(MCMCvis)
library(ggmcmc)
library(bridgesampling)
library(bbsBayes)

# import scripts and custom modules
source("functions.R")

source("jags_models/GARCH.R")
source("jags_models/ARCH.R")
source("jags_models/tstudentGARCH.R")
```


\newpage
```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
\newpage

# 1. Introduction



# 2. The data set

The [Yahoo Finance Bitcoin Historical Data](https://www.kaggle.com/datasets/arslanr369/bitcoin-price-2014-2023) from Kaggle, spanning from 2014 to 2023, capture the evolution of Bitcoin's price over a decade, offering an overview of the following features about Bitcoin value:

```{r results='asis'}
# give a look
head(subset(data, select = -Returns )) %>%gt() %>% tab_options(., table.width = 300)
```

Of the kaggle data set we are solely interested in one of the features: the adjusted closing price of bitcoin (in terms of BTC/USD value).

## 2.1. Returns and prices
In the analysis of financial data, asset equity returns are typically the main variable of interest (rather than prices) There are at least two reasons for this:

1. Returns are easier to interpret
2. Returns have statistical properties which are easier to handle (e.g. stationarity)

Let $P_t$ be the price of an asset at period $t$ $(t = 1, .., T)$ the **simple return** is defined as the gross return:

$$
R_t = \frac{P_t-P_{t-1}}{P_{t-1}}
$$
In other words, $R_t$ is the gross return generated by holding the
asset for one period.
Simple returns are a natural way of measuring the variation of the
value of an asset, however, it is more common to work with
**log returns**, defined as:

$$
\epsilon_t= log(P_t)-log(P_{t-1}) 
$$
It is a good habit to multiply returns by 100 to express them as a percentage. Some statistical packages are sensitive to the scale of the data. Since log differences can be small, sometimes this creates numerical difficulties.

Thus:

$$
\epsilon_t= 100 \cdot (log(P_t)-log(P_{t-1}) )
$$

Below is a graphical comparison of prices and returns: note some evidence of leverage as descending prices imply higher volatility of returns.

Volatility we will therefore see will be a key issue in choosing our models and will influence our inferential results.

\

```{r}
plot_returns <- ggplot(data, aes(x = Date)) +
  geom_line(aes(y = Returns), color = "red") +
  labs(title = "Returns Time Series",
       x = NULL,
       y = "Returns") +
  theme_minimal()

plot_prices <- ggplot(data, aes(x = Date)) +
  geom_line(aes(y = Adj.Close), color = "blue") +
  labs(title = "Prices Time Series",
       x = "Date",
       y = "Price") +
  theme_minimal()

# Combine plots using facet_wrap
ggarrange(plot_returns, plot_prices,
          ncol = 1, nrow = 2)
```

## 2.2. Volatility and heteroscedasticity

The conditional variance of a time series refers to the variability or dispersion of the data points in the series given the past observations or information. It is a measure of how much the values of the time series fluctuate around their conditional mean, taking into account the historical values of the series. In other words, it quantifies the uncertainty or risk associated with future values of the time series given the available information.

Mathematically, if $Y_t$ represents the value of the time series at time $t$ and the conditional variance of $Y_t$ given the past observations up to time $t-1$, then it can be expressed as:

$$
\sigma_t^2 = Var(Y_t|Y_{t-1},Y_{t-2},..., Y_1)
$$
Volatility, on the other hand, is a broader concept that generally refers to the degree of variation or dispersion of a financial asset's price (or return) over time. It measures the magnitude of price fluctuations and is often used to assess the risk or uncertainty associated with an investment. In the context of financial markets, volatility is often calculated using various statistical measures, such as standard deviation or variance, to quantify how much an asset's price tends to deviate from its average.



\

```{r, fig.show="hold", out.width="50%"}
# Plot ACF for Returns
acf.plot(data$LogReturns)
```
The inspection of the autcorrelograms suggests that:

>- returns appear to have weak or no serial dependence
>- absolute (and so square) returns appear to have strong serial dependence

In other words we know that the scale of returns changes in time and the (conditional) variance of the process is time varying. In order to capture **volatility clustering**, we need to introduce
appropriate time series processes able to model this behavior!

We therefore infer that the volatility of the returns is directly symptomatic of a strong heteroscedasticity of the data.

**Heteroscedasticity** is the situation in which the variance of the residuals of a regression model is not the same across all values of the predicted variable. In other words, the variability of the residuals (i.e., error term) increases or decreases over the range of predictions.

We therefore infer that the volatility of returns is directly symptomatic of a strong heteroscedasticity of the data. However, it does not make sense to proceed with the visualization of a linear model (lm) fictitious on our data and/or a hypothesis test (e.g., white test) involving the use of an lm. Later we will perform a test to confirm heteroscedasticity and choose an appropriate model (e.g., ARCH model).

## 2.3. Time series decomposition and stationarity 

A given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.

These components are defined as follows:

>- _Level_: The average value in the series.
>- _Trend_: The increasing or decreasing value in the series.
>- _Seasonality_: The repeating short-term cycle in the series.
>- _Noise (Random)_: The random variation in the series.

A series is thought to be an aggregate or combination of these four components, all series have a level and noise, while The trend and seasonality components are optional.

In our particular case we decide to consider a **multiplicative model** in which the timeseries is thought as a multiplicative combination of the above components:

$$
y(t) = Level \times Trend \times Seasonality \times Noise
$$

We report below the decomposition results for _prices_ and _returns_ considering that we have one observation per day (for 9 years).

```{r}
times <- ts(data$LogReturns, frequency = 365)
decomposed.ts <- decompose(times, type="multiplicative") 
decomp.plot(decomposed.ts, "returns", "red")
```

```{r}
times <- ts(data$Adj.Close, frequency = 365)
decomposed.ts <- decompose(times, type="multiplicative") 
decomp.plot(decomposed.ts, "prices", "blue")
```



### 2.3.1. Level

Simply observing the **Level** for both our _Price_ and _Returns_ variables we can propose some obvious findings:

1. our choice of a multiplicative model is justified by obvious nonlinear changes in the values of both variables, i.e. the changes increase and decrease over time;
2. they do not seem to have obvious trends and seasonality;
3. we can guess perhaps some **heteroscedasticity** in the data given large fluctuations in values for adjacent data.

In the following sections we are going to dive into the analysis of the last 2 reported points.


### 2.3.2. Trend and seasonality

The **Trend** represents the long-term change in the level of a time series. This change can be either upward (increase in level) or downward (decrease in level). If the change is systematic in one direction, then the trend is monotonic.
ARIMA-type models for instance are suitable in case of data with an obvious trend, but cannot handle seasonality.

**Seasonality** refers to periodic fluctuations in certain business areas and cycles that occur regularly based on a particular season. A season may refer to a calendar season such as summer or winter, or it may refer to a commercial season such as the holiday season. 
An extension of ARIMA models to handle seasonality is the SARIMA model.

In this case, however, we find neither trend nor seasonality

### 2.3.3 Stationarity

At first glance (with the simple visualization of the observed levels) and based on the previously obtained results we can assume the **stationarity** of our time series. A stationary time series is one whose properties do not depend on the time at which the series is observed.15 Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. In general, a stationary time series will have no predictable patterns in the long-term.

the Dickey–Fuller test (Dickey & Fuller, 1979) tests the null hypothesis that a unit root is present in an autoregressive (AR) time series model. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity.

Given that A simple AR model is given by:

$$
y_t = \rho y_{t-1} + u_t
$$
where:

>- $y_t$ is the variable of interest (returns) at time $t$;
>- $\rho$ is a coefficient;
>- $u_t$ is an error term.

Write the regression model as:

$$
\Delta y_t = (\rho -1) y_{t-1} + u_t = \delta y_{t-1} + u_t
$$
where $\Delta$ is the first difference operator and $\delta = \rho -1$. This model can be estimated, and testing for a unit root is equivalent to testing $\delta = 0$.

The **augmented Dickey–Fuller (ADF) statistic** (augmented since it removes all the structural  - autocorrelation - effects in the time series), used in the test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.

```{r results='markup'}
adf.test(data$LogReturns[-1])
```
We got evidence against the the null hypothesis, thus we simply verified the stationarity of our data!

This aspect is going to be a central point since the models we are going to use in order to well represent volatility often imply stationarity of the time series.

## 2.4. ARCH effect

We veryfied in point _2.2._ that the time series exhibits conditional heteroscedasticity through the study of the autocorrelation in the absolute/squared series. Now we formally check for the **autoregressive conditional heteroscedastic (ARCH) effects**.

The **Lagrange multiplier (LM) test** for ARCH is widely used as a specification test in univariate time series models. It is a test of no conditional heteroskedasticity against an ARCH model. The test is easy to compute from an auxiliary regression involving the squared least squares (LS) residuals. The LM statistic is asymptotically distributed as $\chi^2$ under the null hypothesis.

```{r, results='markup'}
Lm.test(data$LogReturns[-1])
```
With this result we can claim with a large evidence that our time series is NOT homoscedastic!


# 3. Models

Enunciare caratteristiche che un ipotetico modello deve seguire


## 3.1. The ARCH model

The **ARCH (Autoregressive Conditional Heteroskedasticity)** model (Engle, 1982) is a statistical time series model commonly used in econometrics and finance to capture volatility clustering in financial data. It was introduced by Robert F. Engle in the early 1980s as a way to model the changing volatility observed in financial returns over time. The ARCH model is particularly useful for analyzing financial time series data where the volatility, or the variation in the magnitude of returns, is not constant and can exhibit patterns of clustering or persistence.

The **ARCH(1)** model is given by:

$$
\begin{aligned}
y_t &= \sqrt{\sigma^2_t} \cdot z_t  \nonumber \\
\sigma^2_t &= \omega + \alpha y^2_{t-1}
\end{aligned}
$$

where $w > 0, \alpha \geq 0$ in order to have
stationarity.

This variables and parameters have a specific meaning in our model:

>- $y_t$ is the observed value at time t
>- $z_t$ is the white noise (innovation) term at time t
>- $\sigma_t^2$ is the conditional variance of $y_t$ 
>- $\omega$ is the baseline volatility
>- $\alpha$ represents the impact of past squared residuals on the conditional variance

We then define our _hierarchical bayesian ARCH model_ selecting the correct set of distributions for the above parameters and variable. We use the following setup:

$$
\begin{aligned}
z_t &\sim N(\mu, 1) \rightarrow y_t= \sigma_t z_t \sim N(\mu, \sigma_t^2)  \nonumber \\
\mu &\sim N(0, 100^2) \nonumber \\
\omega &\sim U(0, 10) \nonumber \\
\alpha &\sim U(0, 1)
\end{aligned}
$$
It follows the above model coded with JAGS.

```{r echo=TRUE}
arch_model_code <- "
model
{
  # Likelihood
  for (t in 1:N) {
    y[t] ~ dnorm(mu, tau[t])
    tau[t] <- 1/pow(sigma[t], 2)
  }
  sigma[1] ~ dunif(0,10)
  for(t in 2:N) {
    sigma[t] <- sqrt(omega + alpha * pow(y[t-1] - mu, 2))
  }

  # Priors
  mu ~ dnorm(0.0, 0.01)
  omega ~ dunif(0, 10)
  alpha ~ dunif(0, 1)
}
"
```




## 3.2. The Gaussian GARCH model

The **GARCH (Generalized Autoregressive Conditional Heteroskedasticity)** model (Bollerslev, 1986) is an extension of the ARCH (Autoregressive Conditional Heteroskedasticity) model that further captures and models the time-varying volatility in financial and economic time series data. Introduced by Tim Bollerslev in the mid-1980s, the GARCH model addresses some of the limitations of the basic ARCH model by incorporating past values of the conditional variance itself into the volatility modeling process.

Mathematically, a **GARCH(1,1,)** model is given by the following structure:

$$
\begin{aligned}
y_t &= \sqrt{\sigma^2_t} \cdot z_t  \nonumber \\
\sigma^2_t &= \omega + \alpha y^2_{t-1} + \beta \sigma_{t-1}^2
\end{aligned}
$$

As can be seen, the only difference from the previous model is the dependence of volatility on past volatility values (conditional variance) and the introduction of a new $\beta$ parameter to govern this relationship.

GARCH (Generalized Autoregressive Conditional Heteroskedasticity) and ARCH (Autoregressive Conditional Heteroskedasticity) are both models used to analyze and forecast volatility in financial time series data, such as the volatility of Bitcoin prices. The preference for GARCH over ARCH for modeling Bitcoin volatility is based on several factors:

1. Flexibility and Improved Modeling: GARCH is an extension of the ARCH model that allows for more complex and flexible modeling of volatility dynamics. GARCH models incorporate both lagged conditional variances (as in ARCH) and lagged conditional variances of the squared past returns. This added flexibility often helps capture more intricate volatility patterns observed in financial data like Bitcoin prices.

2. Better Fit to Real Data: Cryptocurrencies like Bitcoin are known for their unique volatility characteristics, including periods of extreme volatility followed by relative stability. GARCH models with their ability to capture changing volatility patterns over time are often better suited to capture these fluctuations and trends in the data.

3. Accommodation of Volatility Clustering: Volatility clustering refers to the phenomenon where periods of high volatility tend to cluster together over time. GARCH models can capture this clustering effect by allowing for the persistence of volatility shocks, making them more suitable for assets like Bitcoin that often exhibit this behavior.

4. More Sophisticated Volatility Forecasting: GARCH models can generate volatility forecasts that are more accurate and reliable compared to ARCH models. This is crucial for risk management and derivative pricing, where accurate volatility forecasts are essential.

5. Statistical Significance and Model Selection: GARCH models often provide more accurate parameter estimates and better model fit, as determined by statistical tests and criteria. This helps in selecting a more appropriate and reliable model for analyzing Bitcoin volatility.

We then define our _hierarchical bayesian GARCH model_ selecting the correct set of distributions for the above parameters and variable. We use the following setup:

$$
\begin{aligned}
z_t &\sim N(\mu, 1) \rightarrow y_t= \sigma_t z_t \sim N(\mu, \sigma_t^2)  \nonumber \\
\mu &\sim N(0, 100^2) \nonumber \\
\omega &\sim U(0, 10) \nonumber \\
\alpha &\sim U(0, 1) \nonumber \\
\beta &\sim U(0, 1)
\end{aligned}
$$

It follows the above model coded with JAGS.

```{r echo=TRUE}
garch_model_code <- "
model
{
  # Likelihood
  for (t in 1:N) {
    y[t] ~ dnorm(mu, tau[t])
    tau[t] <- 1/pow(sigma[t], 2)
  }
  sigma[1] ~ dunif(0,10)
  for(t in 2:N) {
    sigma[t] <- sqrt(omega + alpha * pow(y[t-1] - mu, 2) +
                     beta * pow(sigma[t-1], 2))
  }

  # Priors
  mu ~ dnorm(0.0, 0.01)
  omega ~ dunif(0, 10)
  alpha ~ dunif(0, 1)
  beta ~ dunif(0, 1)
}
"
```

## 3.3. The Student's $t$ GARCH model

We can then begin to analyze the distribution of our data. Let's start with a simple visualization of the distribution (histogram) of our data and then try to fit some possible density functions by selecting a few by intuition and using MLE for parameters.


```{r, figures-side, fig.show="hold", out.width="50%"}
fit_distros(data$Adj.Close)
fit_distros(data$LogReturns[-1], flag = FALSE)
```

We note that fortunately for log-returns a normal distribution seems quite consistent for a possible prior assumption in Bayesian framework.

However, we are not satisfied with a simple visualization of this kind and continue the investigation with a **Kolmogorov-Smirnov test**, a _goodnes of fit_ test. The resulting p-value, however, indicates that the population does not actually exhibit normal distribution, rejecting the null hypothesis.

```{r results='markup'}
ks.test(data$LogReturns[-1], "pnorm", mean = mean(data$LogReturns[-1]),
        sd = sd(data$Returns[-1]))
```
We then disprove our initial assumption of a normal prior and via a _QQ plot_ the obvious differences between the current data and the normal distribution that we were previously unable to capture.

```{r}
ggplot(data, aes(sample=LogReturns)) +
  stat_qq(color="purple") + 
  stat_qq_line() +
  labs(title = "QQ Plot for returns")
```

Traditionally, the errors (and so the returns) have been assumed to be Gaussian, however, it
has been widely acknowledged that financial returns display fat tails and are not conditionally
Gaussian. Gaussian GARCH models cannot quite capture the key properties of asset returns and to address this, researchers (Bollerslev (1987), He & Teräsvirta (1999) and Bai et al. (2003)) have explored alternative distributions to model asset returns.
One approach is to assume that the returns are IID and follow a Student’s t-distribution (t-Student) instead of a normal distribution. The t-Student distribution allows for fatter tails, meaning it assigns higher probabilities to extreme events compared to the normal distribution.

```{r}
ggplot(data, aes(sample = LogReturns)) +
  stat_qq(distribution = qt, dparams = list(df = 8, ncp=0.01), color = "purple") +
  stat_qq_line(distribution = qt, dparams = list(df = 8, ncp=0.01)) +
  labs(title = "QQ Plot for Student's t-distribution")
```


We use a GARCH model with a **non-central t-student** prior distribution for returns:

$$
\begin{aligned}
y_t = \sigma_t z_t &\sim t(\mu, \sigma_t^2, \nu)  \nonumber \\
\mu &\sim N(0, 100^2) \nonumber \\
\omega &\sim U(0, 10) \nonumber \\
\alpha &\sim U(0, 1) \nonumber \\
\beta &\sim U(0, 1)
\end{aligned}
$$

where:

>- $\delta = \mu \cdot \sigma^{- \frac{1}{2}}_t$ is the non-centrality parameter
>- $\nu = 8$ represents the degrees of freedom and we set it equal to a constant according to a study provided in ["Notes on the econometrics of asset allocation and risk measurement"](https://didattica.unibocconi.it/mypage/index.php?IdUte=48917&idr=3008) (Favero)

Here the code:

```{r echo=TRUE}
tstud_model_code <- "
model
{
  # Likelihood
  for (t in 1:N) {
    y[t] ~ dt(mu, tau[t], 8)
    tau[t] <- 1/pow(sigma[t], 2)
  }
  sigma[1] ~ dunif(0,10)
  for(t in 2:N) {
    sigma[t] <- sqrt(omega + alpha * pow(y[t-1] - mu, 2) +
                     beta * pow(sigma[t-1], 2))
  }

  # Priors
  mu ~ dnorm(0.0, 0.01)
  omega ~ dunif(0, 10)
  alpha ~ dunif(0, 1)
  beta ~ dunif(0, 1)
}
"
```

# 4. Model comparision

Spiegare come procederemo per la comparazione tra modelli

## 4.1. Simple inferential findings and results

Results from the Gaussian ARCH(1) model:

```{r results='markup'}
as.data.frame(MCMCsummary(arch_model, 
              params = arch_model_parameters, 
              HPD = TRUE, 
              hpd_prob = 0.95, 
              round = 8))
```

```{r}
par(mfrow = c(1, 3))
hist(arch_model$BUGSoutput$sims.list$omega, breaks = 30, main= "Distribution of omega",
     col="aquamarine", prob=T, xlab="Omega")
grid()
hist(arch_model$BUGSoutput$sims.list$alpha, breaks = 30, main= "Distribution of alpha",
     col="salmon", prob=T, xlab="Alpha")
grid()
hist(arch_model$BUGSoutput$sims.list$sigma, breaks = 50, main= "Distribution of sigma",
     col="lightblue", prob=T, xlab="Sigma")
grid()
```


Results from the Gaussian GARCH(1,1) model:

```{r results='markup'}
as.data.frame(MCMCsummary(garch_model, 
              params = garch_model_parameters, 
              HPD = TRUE, 
              hpd_prob = 0.95, 
              round = 8))
```

```{r}
par(mfrow = c(2, 2))
hist(garch_model$BUGSoutput$sims.list$omega, breaks = 30, main= "Distribution of omega",
     col="aquamarine", prob=T, xlab="Omega")
grid()
hist(garch_model$BUGSoutput$sims.list$alpha, breaks = 30, main= "Distribution of alpha",
     col="salmon", prob=T, xlab="Alpha")
grid()
hist(garch_model$BUGSoutput$sims.list$alpha, breaks = 30, main= "Distribution of beta",
     col="pink", prob=T, xlab="Beta")
grid()
hist(garch_model$BUGSoutput$sims.list$sigma, breaks = 50, main= "Distribution of sigma",
     col="lightblue", prob=T, xlab="Sigma")
grid()
```


Results from the Student's $t$ GARCH(1,1) model:

```{r results='markup'}
as.data.frame(MCMCsummary(tstud_garch_model, 
              params = tstud_model_parameters, 
              HPD = TRUE, 
              hpd_prob = 0.95, 
              round = 8))
```

```{r}
par(mfrow = c(2, 2))
hist(tstud_garch_model$BUGSoutput$sims.list$omega, breaks = 30, main= "Distribution of omega",
     col="aquamarine", prob=T, xlab="Omega")
grid()
hist(tstud_garch_model$BUGSoutput$sims.list$alpha, breaks = 30, main= "Distribution of alpha",
     col="salmon", prob=T, xlab="Alpha")
grid()
hist(tstud_garch_model$BUGSoutput$sims.list$beta, breaks = 30, main= "Distribution of beta",
     col="pink", prob=T, xlab="Beta")
grid()
hist(tstud_garch_model$BUGSoutput$sims.list$sigma, breaks = 50, main= "Distribution of sigma",
     col="lightblue", prob=T, xlab="Sigma")
grid()
```


## 4.2. Comparision through DIC

```{r}
# Create a dataframe
model_comparison <- data.frame(
  Model = c("ARCH", "GARCH", "t Student GARCH"),
  DIC = c(arch_model$BUGSoutput$DIC , garch_model$BUGSoutput$DIC,
          tstud_garch_model$BUGSoutput$DIC),
  n.chains = c(arch_model$BUGSoutput$n.chains , garch_model$BUGSoutput$n.chains,
          tstud_garch_model$BUGSoutput$n.chains),
  n.iter = c(arch_model$BUGSoutput$n.iter , garch_model$BUGSoutput$n.iter,
          tstud_garch_model$BUGSoutput$n.iter),
  pD = c(arch_model$BUGSoutput$pD , garch_model$BUGSoutput$pD,
          tstud_garch_model$BUGSoutput$pD))
model_comparison %>%gt() %>% tab_options(., table.width = 500)
```


## 4.3. Comparision through Marginal Likelihood


```{r}
# Function to calculate log posterior for ARCH model
# log_posterior_ARCH <- function(samples.row, data) {
#   mu <- samples.row[, "mu"]
#   omega <- samples.row[, "omega" ]
#   alpha <- samples.row[,"alpha"]
#   sigma <- sqrt(omega + alpha * (data$y[-1] - mu)^2)
#   
#   res <- sum(dnorm(data$y, mu, sigma, log = TRUE)) +
#     dunif(omega, 0, 10, log = TRUE) +
#     dunif(alpha, 0, 1, log = TRUE) +
#     dnorm(mu, 0.0, 0.01, log = TRUE)
#   return(res)
# }
# 
# # Function to calculate log posterior for GARCH model
# log_posterior_GARCH <- function(samples.row, data) {
#   mu <- samples.row[, "mu" ]
#   sigma <- samples.row[ paste0("sigma[", seq_along(data$y), "]") ]
#   omega <- samples.row[[ "omega" ]]
#   alpha <- samples.row[[ "alpha" ]]
#   beta <- samples.row[[ "beta" ]]
#   
#   sum(dnorm(data$y, mu, 1/sqrt(sigma^2), log = TRUE)) +
#     dunif(sigma[1], 0, 10, log = TRUE) +
#     sum(dnorm(sigma[2:length(sigma)], sqrt(omega + alpha * (data$y[-1] - mu)^2 + beta * sigma[1:(length(sigma)-1)]^2), log = TRUE)) +
#     dnorm(mu, 0.0, 0.01, log = TRUE) +
#     dunif(omega, 0, 10, log = TRUE) +
#     dunif(alpha, 0, 1, log = TRUE) +
#     dunif(beta, 0, 1, log = TRUE)
# }
```

```{r}
# Specify parameter bounds for ARCH model (H0)
# cn_ARCH <- colnames(arch_model$BUGSoutput$sims.matrix)
# cn_ARCH <- cn_ARCH[cn_ARCH != "deviance"]
# lb_ARCH <- rep(-Inf, length(cn_ARCH))
# ub_ARCH <- rep(Inf, length(cn_ARCH))
# names(lb_ARCH) <- names(ub_ARCH) <- cn_ARCH
# lb_ARCH[["sigma[1]"]] <- 0
# 
# # Specify parameter bounds for GARCH model (H1)
# cn_GARCH <- colnames(garch_model$BUGSoutput$sims.matrix)
# cn_GARCH <- cn_GARCH[cn_GARCH != "deviance"]
# lb_GARCH <- rep(-Inf, length(cn_GARCH))
# ub_GARCH <- rep(Inf, length(cn_GARCH))
# names(lb_GARCH) <- names(ub_GARCH) <- cn_GARCH
# lb_GARCH[["sigma[1]"]] <- 0
```

```{r}
# # compute log marginal likelihood via bridge sampling for H0
# H0.bridge <- bridge_sampler(samples = arch_model, data = arch_model_data,
#                             log_posterior = log_posterior_ARCH, lb = lb_ARCH,
#                             ub = ub_ARCH, silent = FALSE)
# print(H0.bridge)
```

```{r}
# compute log marginal likelihood via bridge sampling for H0
# H0.bridge <- bridge_sampler(samples = arch_model, data = arch_model_data,
#                             log_posterior = log_posterior_ARCH, lb = lb_ARCH,
#                             ub = ub_ARCH, silent = TRUE)
# print(H0.bridge)
```


# 5. Convergence analysis

```{r}
CPR.ggs <- ggs(as.mcmc(garch_model)) # convert to ggs object
plot1 <- ggs_traceplot(CPR.ggs, family = "mu")
plot2 <- ggs_traceplot(CPR.ggs, family = "alpha")
plot3 <- ggs_traceplot(CPR.ggs, family = "beta")
grid.arrange(plot1, plot2, plot3, ncol = 1, nrow = 3)
```

```{r}

```


# Inference

# Forecasting

```{r}
tail(data$LogReturns)
```


# Comparative analysis with frequentist inference

# Second model

Denote by $I_{t-1}$ the information set observed up to time $t-1$, that is, $I_{t-1} = \{y_{t-1}, i > 0\}$. The general **Markov-switching GARCH** specification can then be expressed as:

$$
y_t|(s_t=k,I_{t-1}) \sim D(0, h_{k,t, \xi_k})
$$
where $D(0, h_{k,t, \xi_k})$ $D(0, h_{k,t,\xi_k})$ is a continuous distribution with zero mean, time-varying variance $h_{k,t,\xi_k}$ and additional shape parameters gathered in the vector $\xi_k$. The integer-valued stochastic variable $s_t$, defined on the discrete space $\{1,...,K\}$, characterizes the Markov-switching
GARCH model. 

We define the standardized innovations as $n_{k,t} := y_t/\sqrt{h_{z,t}} \overset{\mathrm{iid}}{\sim} D(0,1,\xi_k)$

```{r}
# Initialize a matrix to store the last 30 'y' values
last_30_y <- matrix(NA, nrow = 30, ncol = length(arch_model$BUGSoutput$sims.list$y[[1]]))

# Loop through the last 30 iterations and extract 'y'
total_iterations <- length(arch_model$BUGSoutput$sims.list$y)
for (i in 1:26) {
  last_30_y[i, ] <- arch_model$BUGSoutput$sims.list$y[[total_iterations - 30 + i]]
}

new_data <- read.csv("BTC-USD2.csv")
new_data$LogReturns <- c(NA, eval.log.returns(new_data$Adj.Close))

N1 <- length(new_data$LogReturns)
true_ret <- tail(new_data$LogReturns, 26)
last_30_y_df <- data.frame(pred=last_30_y[1:26], true=true_ret)
```

