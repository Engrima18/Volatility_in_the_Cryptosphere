---
title: | 
  | **Bitcoin volatility analysis**
  | 
author: | 
  | _Author_:
  | Enrico Grimaldi, 1884443
  |
  | _Professor_:
  | Luca Tardella
  |
date: | 
  |
  | La Sapienza University of Rome
  | a.y. 2022/2023
  |
  | Final project for the course of
  | _Statistical Methods for Data Science 2_
output: 
  pdf_document: default
  html_document: default
toc: no
number_sections: true
bibliography: bibliography.bib
biblio-style: "apalike"
fontsize: 12pt
header-includes:
  - \usepackage{adjustbox}
  - \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, results = "hide")
```

```{r}
# import libraries 
library(tidyverse)
library(gt)
library(summarytools)
library(gridExtra)
library(ggplot2)
library(corrplot)
library(reshape2)
library(mcmcplots)
library(caret)
library(ggpubr)
library(fitdistrplus)
library(viridis)
library(tseries)

# import scripts and custom modules
# source("jags_models/GARCH.R")
source("functions.R")
# source("jags_models/MS-GARCH.R")
```


\newpage
```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
\newpage

# 1. Introduction



# 2. The data set

The [Yahoo Finance Bitcoin Historical Data](https://www.kaggle.com/datasets/arslanr369/bitcoin-price-2014-2023) from Kaggle, spanning from 2014 to 2023, capture the evolution of Bitcoin's price over a decade, offering an overview of the following features about Bitcoin value:

```{r results='asis'}
# give a look
head(subset(data, select = -Returns )) %>%gt() %>% tab_options(., table.width = 300)
```

Of the kaggle data set we are solely interested in one of the features: the adjusted closing price of bitcoin (in terms of BTC/USD value).

## 2.1. Returns rather than prices
In the analysis of financial data, asset equity returns are typically the main variable of interest (rather than prices) There are at least two reasons for this:

1. Returns are easier to interpret
2. Returns have statistical properties which are easier to handle (e.g. stationarity)

Let $P_t$ be the price of an asset at period $t$ $(t = 1, .., T)$ the **simple return** is defined as the gross return:

$$
R_t = \frac{P_t-P_{t-1}}{P_{t-1}}
$$
In other words, $R_t$ is the gross return generated by holding the
asset for one period.
Simple returns are a natural way of measuring the variation of the
value of an asset, however, it is more common to work with
**log returns**, defined as:

$$
\epsilon_t= log(P_t)-log(P_{t-1}) 
$$
It is a good habit to multiply returns by 100 to express them as a percentage. Some statistical packages are sensitive to the scale of the data. Since log differences can be small, sometimes this creates numerical difficulties.

Thus:

$$
\epsilon_t= 100 \cdot (log(P_t)-log(P_{t-1}) )
$$

Below is a graphical comparison of prices and returns: note some evidence of leverage as descending prices imply higher volatility of returns.

Volatility we will therefore see will be a key issue in choosing our models and will influence our inferential results.

\

```{r}
plot_returns <- ggplot(data, aes(x = Date)) +
  geom_line(aes(y = Returns), color = "red") +
  labs(title = "Returns Time Series",
       x = NULL,
       y = "Returns") +
  theme_minimal()

plot_prices <- ggplot(data, aes(x = Date)) +
  geom_line(aes(y = Adj.Close), color = "blue") +
  labs(title = "Prices Time Series",
       x = "Date",
       y = "Price") +
  theme_minimal()

# Combine plots using facet_wrap
ggarrange(plot_returns, plot_prices,
          ncol = 1, nrow = 2)
```

## 2.2. Conditional variance and volatility

The conditional variance of a time series refers to the variability or dispersion of the data points in the series given the past observations or information. It is a measure of how much the values of the time series fluctuate around their conditional mean, taking into account the historical values of the series. In other words, it quantifies the uncertainty or risk associated with future values of the time series given the available information.

Mathematically, if $Y_t$ represents the value of the time series at time $t$ and the conditional variance of $Y_t$ given the past observations up to time $t-1$, then it can be expressed as:

$$
\sigma_t^2 = Var(Y_t|Y_{t-1},Y_{t-2},..., Y_1)
$$
Volatility, on the other hand, is a broader concept that generally refers to the degree of variation or dispersion of a financial asset's price (or return) over time. It measures the magnitude of price fluctuations and is often used to assess the risk or uncertainty associated with an investment. In the context of financial markets, volatility is often calculated using various statistical measures, such as standard deviation or variance, to quantify how much an asset's price tends to deviate from its average.

\

```{r, fig.show="hold", out.width="50%"}
# Plot ACF for Returns
acf.plot(data$LogReturns)
```
The inspection of the autcorrelograms suggests that:

>- returns appear to have weak or no serial dependence
>- absolute and square returns appear to have strong serial dependence

In other words we know that the scale of returns changes in time and the (conditional) variance of the process is time varying. In order to capture **volatility clustering**, we need to introduce
appropriate time series processes able to model this behavior!





## 2.3. Decomposing timeseries

A given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.

These components are defined as follows:

>- _Level_: The average value in the series.
>- _Trend_: The increasing or decreasing value in the series.
>- _Seasonality_: The repeating short-term cycle in the series.
>- _Noise (Random)_: The random variation in the series.

A series is thought to be an aggregate or combination of these four components, all series have a level and noise, while The trend and seasonality components are optional.

In our particular case we decide to consider a **multiplicative model** in which the timeseries is thought as a multiplicative combination of the above components:

$$
y(t) = Level \times Trend \times Seasonality \times Noise
$$

We report below the decomposition results for _prices_ and _returns_ considering that we have one observation per day (for 9 years).

```{r}
times <- ts(data$LogReturns, frequency = 365)
decomposed.ts <- decompose(times, type="multiplicative") 
decomp.plot(decomposed.ts, "returns", "red")
```

```{r}
times <- ts(data$Adj.Close, frequency = 365)
decomposed.ts <- decompose(times, type="multiplicative") 
decomp.plot(decomposed.ts, "prices", "blue")
```



### 2.3.1. Level

Simply observing the **Level** for both our _Price_ and _Returns_ variables we can propose some obvious findings:

1. our choice of a multiplicative model is justified by obvious nonlinear changes in the values of both variables, i.e. the changes increase and decrease over time;
2. they do not seem to have obvious trends and seasonality;
3. we can guess perhaps some **heteroscedasticity** in the data given large fluctuations in values for adjacent data.

In the following sections we are going to dive into the analysis of the last 2 reported points.


### 2.3.2. Trend and seasonality

The **Trend** represents the long-term change in the level of a time series. This change can be either upward (increase in level) or downward (decrease in level). If the change is systematic in one direction, then the trend is monotonic.
ARIMA-type models for instance are suitable in case of data with an obvious trend, but cannot handle seasonality.

**Seasonality** refers to periodic fluctuations in certain business areas and cycles that occur regularly based on a particular season. A season may refer to a calendar season such as summer or winter, or it may refer to a commercial season such as the holiday season. 
An extension of ARIMA models to handle seasonality is the SARIMA model.

In this case, however, we find neither trend nor seasonality


## 2.4. Searching for a prior

We can then begin to analyze the distribution of our data. Let's start with a simple visualization of the distribution (histogram) of our data and then try to fit some possible density functions by selecting a few by intuition and using MLE for parameters.


```{r, figures-side, fig.show="hold", out.width="50%"}
fit_distros(data$Adj.Close)
fit_distros(data$LogReturns[-1], flag = FALSE)
```

We note that fortunately for log-returns a normal distribution seems quite consistent for a possible prior assumption in Bayesian framework.


```{r}
ks.test(data$Returns[-1], "pnorm", mean = mean(data$LogReturns[-1]),
        sd = sd(data$Returns[-1]))
```


```{r}
ggplot(data, aes(sample=LogReturns)) +
  stat_qq(color="purple") + 
  stat_qq_line() +
  labs(title = "QQ Plot for returns")
```


# 3. First model

## 3.1. ARCH vs GARCH

### 3.1.1. The ARCH model


The **ARCH (Autoregressive Conditional Heteroskedasticity)** model is a statistical time series model commonly used in econometrics and finance to capture volatility clustering in financial data. It was introduced by Robert F. Engle in the early 1980s as a way to model the changing volatility observed in financial returns over time. The ARCH model is particularly useful for analyzing financial time series data where the volatility, or the variation in the magnitude of returns, is not constant and can exhibit patterns of clustering or persistence.
The **ARCH(1)** model is given by:

$$
y_t = \sqrt{\sigma^2_t} \cdot z_t
$$
$$
z_t \sim D(0,1)
$$

where $D$ is a distribution with mean 0 and variance 1 and in our case $D(0,1) = N(0,1)$. This implies that $y_t = N(\mu, \sigma^2_t)$ and

$$
\sigma^2_t= \omega + \alpha y^2_{t-1}
$$

where $w > 0, \beta > 0, \alpha+\beta < 1$ (in order to have
stationarity).

This variables and parameters have a specific meaning in our model:

>- $y_t$ is the observed value at time t
>- $z_t$ is the white noise (innovation) term at time t
>- $\sigma_t^2$ is the conditional variance of $y_t$ 
>- $\omega$ is the baseline volatility
>- $\alpha$ represents the impact of past squared residuals on the conditional variance

### 3.1.2. The GARCH model

The **GARCH (Generalized Autoregressive Conditional Heteroskedasticity)** model is an extension of the ARCH (Autoregressive Conditional Heteroskedasticity) model that further captures and models the time-varying volatility in financial and economic time series data. Introduced by Tim Bollerslev in the mid-1980s, the GARCH model addresses some of the limitations of the basic ARCH model by incorporating past values of the conditional variance itself into the volatility modeling process.

Mathematically, a **GARCH(1,1,)** model is given by the following structure:
$$
y_t = \sqrt{\sigma^2_t} \cdot z_t
$$


$$
z_t \sim D(0,1)
$$

$$
\sigma^2_t = \omega + \alpha\epsilon^2_{t-1} + \beta\sigma^2_{t-1}
$$
As can be seen, the only difference from the previous model is the dependence of volatility on past volatility values (conditional variance) and the introduction of a new $\beta$ parameter to govern this relationship.

GARCH (Generalized Autoregressive Conditional Heteroskedasticity) and ARCH (Autoregressive Conditional Heteroskedasticity) are both models used to analyze and forecast volatility in financial time series data, such as the volatility of Bitcoin prices. The preference for GARCH over ARCH for modeling Bitcoin volatility is based on several factors:

1. Flexibility and Improved Modeling: GARCH is an extension of the ARCH model that allows for more complex and flexible modeling of volatility dynamics. GARCH models incorporate both lagged conditional variances (as in ARCH) and lagged conditional variances of the squared past returns. This added flexibility often helps capture more intricate volatility patterns observed in financial data like Bitcoin prices.

2. Better Fit to Real Data: Cryptocurrencies like Bitcoin are known for their unique volatility characteristics, including periods of extreme volatility followed by relative stability. GARCH models with their ability to capture changing volatility patterns over time are often better suited to capture these fluctuations and trends in the data.

3. Accommodation of Volatility Clustering: Volatility clustering refers to the phenomenon where periods of high volatility tend to cluster together over time. GARCH models can capture this clustering effect by allowing for the persistence of volatility shocks, making them more suitable for assets like Bitcoin that often exhibit this behavior.

4. More Sophisticated Volatility Forecasting: GARCH models can generate volatility forecasts that are more accurate and reliable compared to ARCH models. This is crucial for risk management and derivative pricing, where accurate volatility forecasts are essential.

5. Statistical Significance and Model Selection: GARCH models often provide more accurate parameter estimates and better model fit, as determined by statistical tests and criteria. This helps in selecting a more appropriate and reliable model for analyzing Bitcoin volatility.


# 4. Second model

Denote by $I_{t-1}$ the information set observed up to time $t-1$, that is, $I_{t-1} = \{y_{t-1}, i > 0\}$. The general **Markov-switching GARCH** specification can then be expressed as:

$$
y_t|(s_t=k,I_{t-1}) \sim D(0, h_{k,t, \xi_k})
$$
where $D(0, h_{k,t, \xi_k})$ $D(0, h_{k,t,\xi_k})$ is a continuous distribution with zero mean, time-varying variance $h_{k,t,\xi_k}$ and additional shape parameters gathered in the vector $\xi_k$. The integer-valued stochastic variable $s_t$, defined on the discrete space $\{1,...,K\}$, characterizes the Markov-switching
GARCH model. 

We define the standardized innovations as $n_{k,t} := y_t/\sqrt{h_{z,t}} \overset{\mathrm{iid}}{\sim} D(0,1,\xi_k)$
